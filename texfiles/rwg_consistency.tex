%!TEX root=../rwg.tex
\section{Consistency vs. Performance in BigData Systems}
A recurring discussion theme during the cloud and big data systems working
group was data consistency.  There has been much research related to
characterizing and tweaking the level of consistency in data storage and
processing systems.  However, big data poses novel challenges which make
questions of consistency especially tricky in contemporary systems.  First,
building strongly consistency storage systems is difficult due to the velocity
of data generated by modern big data applications.  For example, Facebook
generates 500 TB of data per day at multiple datacenters around the
world~\cite{fb_size}.  It is non-trivial to store all such data and guarantee
that applications will see a consistent time-line of data accesses.  Second, the
data in such modern applications is varied, and the variety poses challenges in
coming up with a uniform schema that supports strong consistency.  Third, the
amount of data, by definition, is huge, and traditional database techniques for
consistently storing and processing data do not serve such a large volume of
data with acceptable performance.  There was consensus among all present that
we need to solve the tension between strong consistency and high performance in
big data systems.

The root cause of this tension in big data systems is that in order to support
the large volume and velocity of data and not perform at a snail's pace, such
systems have to inevitably be either multi-core or distributed or both.  Because
the data can be manipulated by multiple threads, at multiple machines, and often
in multiple datacenters, ensuring consistency is a huge challenge.  The rich
body of traditional database literature does not serve such data well.  For
example, executing a database transaction by performing two-phase locking across
datacenters will lead to very high commit latency.

Of course, enabling fast and consistent systems is a classic problem for
systems researchers.  Recent work has attempted to weaken the consistency
guarantees of the system in order to achieve higher performance.  Most of these
systems have been advised by the so-called CAP theorem~\cite{cap}.
Essentially, the CAP theorem explains that in an environment that permits
unconstrained system crash failures and network partitions, it is impossible to
build a strongly consistent system.  The CAP theorem is true, but not
particularly interesting because in practice we can make stronger assertions
about modern networks and distributed systems.  Unfortunately, it has been
interpreted by the designers of many contemporary systems to mean that we
necessarily need to abandon one out of three desirable system
properties---consistency, availability, and partition-tolerance.  Thus, many
storage systems such as MongoDB have preemptively given up on consistency in
order to achieve better performance.  Other systems have posited progressive
weakening of the consistency guarantee, leading to a whole host of consistency
levels with different definitions~\cite{vogels_consistency}.  It is very
difficult for application programmers to understand a variety of consistency
models and write bug-free code against such systems.

The working group on cloud and big data systems was of the opinion that we need
to build systems that offer strong consistency from the ground up.  By
incorporating strong consistency as a basic system design goal, we will ensure
that the users of such big data storage and processing systems can easily
understand the system semantics.  This will minimize the number of bugs due to
overestimation of the system capabilities.  For example, a database that
provides a transactional programming interface is much easier for programmers
to understand.

The key challenge in ensuring that such strongly consistent systems also
provide high performance, in terms of latency, throughput, scalability, etc.  A
lot of recent systems research has been focusing on new techniques for enabling
strong properties without weakening system guarantees.
HyperDex~\cite{HyperDex} is a document store that provides a transactional
interface while enabling better performance than MongoDB, an eventually
consistent database.  FaRM~\cite{farm} leverages new advances in hardware, such
as RDMA and non-volatile DRAM to give high throughput database transactions.
TAPIR~\cite{tapir} and Callas~\cite{callas} implement novel transaction
processing protocols to provide improved performance compared to conventional
database systems on large volume of data.  Combined with many other recent
research works, these novel techniques show that we can achieve our goal of
strong consistency on big data.

While building strongly consistent data systems within a datacenter is
challenging and has been the focus of a lot of recent research, the working
group also accepted that we do not yet have a panacea and there are many open
problems.  Providing strongly consistent operations across datacenters still
remains challenging.  Intelligent transaction protocol design only takes us so
far in geo-replicated applications, since the network latency poses a
fundamental barrier to the overall system performance.  Even within a
datacenter, a single application can often be served by a number of different
databases, and consistently coordinating updates and reads across the different
databases is an open problem.  For such applications, which access data across
multiple database instances, potentially across many datacenters, we need to
build better protocols in concert with novel techniques across the stack such
as better network primitives and intelligent scheduling algorithms.
