%!TEX root=../rwg.tex
\section{Towards better performance guarantees for distributed systems}

\paragraph{}
Over the last decade there has been tremendous progress in the scalability and reliability of distributed systems.
New techniques for distributed transactions have allowed data-stores to scale across multiple servers~\cite{aguilera2007sinfonia}, data centers, and even continents~\cite{Lloyd:2011:DSE:2043556.2043593}.  At the same time, improved replication techniques~\cite{van2004chain} have allowed for better reliability and fault-tolerant centralized services.

However, there is still a disconnect between the proven correctness of these techniques and their actual implementation. While projects like IronFleet~\cite{hawblitzel2015ironfleet} allow formal verification of simple distributed systems, there is still a long road ahead until such an approach can be applied to large-scale systems.
One of the key reasons for this is that, while properties such as liveness, safety and eventual consistencies have been formally proven~\cite{floyd1967assigning}, limited work has been done on providing any form of formal guarantees about the \emph{performance} of a system. It is also hard to provide any empirical guarantees on performance in large-scale distributed systems, which are difficult to debug and reason about.

Given how important performance is,  both for user satisfaction and to the service providers for generating revenues, we believe that the lack of such theoretical or empirical performance guarantees is a gaping hole in the distributed systems literature. As a preliminary attempt towards filling this hole, we discussed two proposals: the first one deals with the theoretical aspects of the problem and the second one deals with it empirically. We elaborate both of these proposals below.

\paragraph{Theoretical - Providing Formal Performance Guarantees:} An interesting approach would be to extend formal methods to include guarantees about performance. A feasible method to achieve this could be to estimate the number of iterations it takes for a system to converge, either using formal verification or mathematical modeling. However, as per the FLP~\cite{fischer1985impossibility} result, this would only be possible, if stronger guarantees are already given about the network performance. The work by Hawblitzel et al.~\cite{hawblitzel2015ironfleet} already built there liveness proofs on such bounds. Arguing with such worst-case bounds, however, would yield in very weak guarantees (e.g. high latency bounds). This means, we need to improve this practice or we will need to find another way to argue about time-bounds in a formally verified system.

\paragraph{Empirical - Benchmarking a distributed system:} We believe that there is a need for easier benchmarking and testing of a distributed system. Automated testing is already prevalent on a single machine. A developer can just run "make check" or similar tools to ensure their changes did not introduce any new bugs. However, for large-scale distributed systems, this is not as trivial. What if multiple concurrent failures occur? How does the system behave if a network link go down?
Further, it is often nontrivial to estimate how a deployment affects performance. It might not be clear for an administrator how (and if) to shard data.

We believe that it might be possible to use existing techniques for virtualization and software-defined networks (SDNs), to deploy and test a distributed system. This would also make it easier for the research community to set up and repeat experiments.
One might, for example, want a set of scripts that automatically creates multiple containers running instances of MongoDB. They can then introduce failure of several of the nodes at a specific point in time, to stress test the system. Without the use of automation, this would be hard to trigger at the exact intended time and even harder to reproduce. In this case using virtual machines will make it much easier to introduce such a failure deterministically. SDNs, on the other hand, can aid in simulating link failures. This can, thus, lead to a common set of benchmarks and evaluation suites, changing the way we conduct systems research significantly. One of the key challenges here would be to ensure that such a virtualized setup adds minimal overheads to the system performance and does not introduce any skew in the results.
